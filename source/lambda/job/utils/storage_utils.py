"""
Helper functions for storage intermediate content or log
"""

import datetime
import json
import logging
from urllib.parse import urlparse
from botocore.exceptions import ClientError

from langchain.docstore.document import Document

logger = logging.getLogger()
logger.setLevel(logging.INFO)


def upload_chunk_to_s3(
    s3, content: str, bucket: str, prefix: str, splitting_type: str, file_type: str
):
    """Upload the logger file to S3 with hierarchy below:
    filename A
        ├── semantic-splitting (split by headers)
        │   ├── timestamp 1
        │   │   ├── logger file 1
        │   ├── timestamp 2
        │   │   ├── logger file 2
        ├── chunk-size-splitting (split by chunk size)
        │   ├── timestamp 3
        │   │   ├── logger file 3
        │   ├── timestamp 4
        │   │   ├── logger file 4
        ├── before-splitting (whole markdown content before splitting)
        │   ├── timestamp 5
        │   │   ├── logger file 5
        │   ├── timestamp 6
        │   │   ├── logger file 6
        ├── qa-enhancement (QA enhanced content generated by LLM)
        │   ├── timestamp 7
        │   │   ├── logger file 7
        │   ├── timestamp 8
        │   │   ├── logger file 8
        ...
    """
    # round the timestamp to hours to avoid too many folders
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d-%H")
    # make the file name unique
    extension = "log" if file_type == "content" else "json"
    object_key = f"{prefix}/{splitting_type}/{timestamp}/{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')}-{file_type}.{extension}"
    try:
        res = s3.put_object(Bucket=bucket, Key=object_key, Body=content)
        logger.debug(f"Upload {file_type} file to S3: {res}")
    except Exception as e:
        logger.error(f"Error uploading {file_type} file to S3: {e}")


def save_content_to_s3(s3, document: Document, res_bucket: str, splitting_type: str):
    """Save content to S3 bucket

    Args:
        document (Document): The page document to be saved
        res_bucket (str): Target S3 bucket
        s3 (_type_): S3 client
    """
    # Extract the filename from the file_path in the metadata
    file_path = document.metadata.get("file_path", "")
    filename = file_path.split('/')[-1].rsplit('.', 1)[0]
    
    # Upload page content as log file
    upload_chunk_to_s3(s3, document.page_content, res_bucket, filename, splitting_type, "content")
    
    # Upload metadata as json file
    metadata_json = json.dumps(document.metadata, ensure_ascii=False)
    upload_chunk_to_s3(s3, metadata_json, res_bucket, filename, splitting_type, "metadata")


def _s3_uri_exist(s3_client, s3_uri: str) -> bool:
    """Checks if an object exists at a given S3 URI. 
    eg. s3://bucket/folder/file.csv

    Args:
        s3_uri: s3 URI to the s3 object

    Returns:
        bool: whether the object exists or not
    """
    parsed = urlparse(s3_uri)

    try:
        s3_client.head_object(Bucket=parsed.netloc, Key=parsed.path.lstrip('/'))
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "404":
            return False
        else:
            raise Exception("Failed to get S3 object during ETL inference")
